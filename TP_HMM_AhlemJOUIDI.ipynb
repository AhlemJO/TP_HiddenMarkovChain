iskander soltani :D 
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> <font color='red'> Text segmentation using Hidden Markov Models </font>  </center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Student :</b> Ahlem JOUIDI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Task : </b> automatic segmentation of mails, problem statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Librairies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "import pandas \n",
    "import seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>The HMM model : </b> HMM (A;B; $\\pi$) with two states, one (state 1) for the header, the other (state 2) for the body. In this model, it is assumed that each mail actually contains a header : the decoding necessarily begins in the state 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1 : Give the value of the $\\pi$ vector of the initial probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowing that the decoding begins necessarily in the state 1. We conclude that the Ï€ vector of the initial probabilities will be : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $$ \\pi = [1,0] $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pi = np.array([1,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2 : What is the probability to move from state 1 to state 2 ? What is the probability to remain in state 2 ? What is the lower/higher probability ? Try to explain why"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowing that each mail contains exactly one header and one body, each mail follows once the transition from 1 to 2. The transition matrix $(A(i; j) = P(j/i)) $ estimated on a labeled small corpus has thus the following form :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The transition matrix A \n",
      "[[9.99218078e-01 7.81921964e-04]\n",
      " [0.00000000e+00 1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "#The transition matrix A \n",
    "print('The transition matrix A ')\n",
    "A = np.array([[0.999218078035812, 0.000781921964187974], [0, 1]])\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Using the transition matrix :\n",
    "\n",
    "     the probability to move from state 1 to state 2 $P(2/1) = 0.000781921964187974   $  \n",
    "     The probalility to remain in state 2 $P(2/2) = 1 $ \n",
    "     \n",
    "-The higher probability is to remain in state 2 because once we enter to the body we can't refind the header. \n",
    "\n",
    "-The lower probability is to move from from state 2 to state 1 because, as I aleardy said once we are in the body it's impossible to retun to state 1 (the header). $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A mail is represented by a sequence of characters. Let N be the number of different characters. Each part of the mail is characterized by a discrete probability distribution on the characters P(c/s), with s = 1 or s = 2.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q3 : What is the size of B?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B is the matrix of probabilities to have the character k at step i $B[k,i]=P(c=k / s=i)$\n",
    "\n",
    "We have N characters and 2 steps ==> So the size of B is N*2.\n",
    "\n",
    "Since we will work with ASCII characters <b> the matrix B  would be of  shape 256 * 2 </b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading necessary files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = np.fromfile('dat/mail1.dat',dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.57145322e-076 3.69722024e-057 8.54943213e-072 1.65609952e-047\n",
      " 1.26931702e-076 9.34288662e-067 2.10775176e-052 2.15664990e-048\n",
      " 3.21866557e-057 3.76366927e+179 2.17462751e-076 1.48744112e-076\n",
      " 7.25416364e-043 2.90191403e-057 6.23126357e-038 3.82847678e-086\n",
      " 3.11774661e-033 5.20461289e-090 8.15762487e-043 2.90130635e-057\n",
      " 1.88517055e-259 1.57145322e-076 3.69722024e-057 8.54943213e-072\n",
      " 1.65609952e-047 1.26931702e-076 9.34288662e-067 2.10775176e-052\n",
      " 2.15664990e-048 3.21866557e-057 3.76366927e+179 2.17462751e-076\n",
      " 1.48744112e-076 7.25416364e-043 2.90191403e-057 6.23126357e-038\n",
      " 3.82847678e-086 3.11774661e-033 5.20461289e-090 8.15762487e-043\n",
      " 2.90130635e-057 1.88517055e-259 1.57145322e-076 3.69722024e-057\n",
      " 8.54943213e-072 1.65609952e-047 1.26931702e-076 1.79453548e-052\n",
      " 6.23703021e-038 8.54943208e-072 1.65609952e-047 1.26931702e-076\n",
      " 7.10943139e-038 1.91857619e-076 1.30352776e-076 4.01521147e-057\n",
      " 1.64318952e-259 1.57145322e-076 3.69722024e-057 8.54943213e-072\n",
      " 1.65609952e-047 1.26931702e-076 9.34288662e-067 2.10775176e-052\n",
      " 2.15664990e-048 3.21866557e-057 3.76366927e+179 2.17462751e-076\n",
      " 1.48744112e-076 7.25416364e-043 2.90191403e-057 6.23126357e-038\n",
      " 3.82847678e-086 3.11774661e-033 5.20461289e-090 8.15762487e-043\n",
      " 2.90130635e-057 1.88517055e-259 1.57145322e-076 3.69722024e-057\n",
      " 8.54943213e-072 1.65609952e-047 1.26931702e-076 9.34288662e-067\n",
      " 2.10775176e-052 2.15664990e-048 3.21866557e-057 3.76366927e+179\n",
      " 2.17462751e-076 1.48744112e-076 7.25416364e-043 2.90191403e-057\n",
      " 6.23126357e-038 3.82847678e-086 3.11774661e-033 5.20461289e-090\n",
      " 8.15762487e-043 2.90130635e-057 1.88517055e-259 1.57145322e-076\n",
      " 3.69722024e-057 8.54943213e-072 1.65609952e-047 1.26931702e-076\n",
      " 9.34288662e-067 2.10775176e-052 2.15664990e-048 3.21866557e-057\n",
      " 3.76366927e+179 2.17462751e-076 1.48744112e-076 7.25416364e-043\n",
      " 2.90191403e-057 6.23126357e-038 3.82847678e-086 3.11774661e-033\n",
      " 5.20461289e-090 8.15762487e-043 2.90130635e-057 1.88517055e-259\n",
      " 1.57145322e-076 3.69722024e-057 8.54943213e-072 1.65609952e-047\n",
      " 1.26931702e-076 9.34288662e-067 2.10775176e-052 2.15664990e-048\n",
      " 3.21866557e-057 3.76366927e+179 2.17462751e-076 1.48744112e-076\n",
      " 7.25416364e-043 2.90191403e-057 6.23126357e-038 3.82847678e-086\n",
      " 3.11774661e-033 5.20461289e-090 8.15762487e-043 2.90130635e-057\n",
      " 1.88517055e-259 1.57145322e-076 3.69722024e-057 8.54943213e-072\n",
      " 1.65609952e-047 1.26931702e-076 9.34288662e-067 2.10775176e-052\n",
      " 2.15664990e-048 3.21866557e-057 3.76366927e+179 2.17462751e-076\n",
      " 1.48744112e-076 7.25416364e-043 2.90191403e-057 6.23126357e-038\n",
      " 3.82847678e-086 3.11774661e-033 5.20461289e-090 8.15762487e-043\n",
      " 2.90130635e-057 1.88517055e-259 1.57145322e-076 3.69722024e-057\n",
      " 8.54943213e-072 1.65609952e-047 1.26931702e-076 1.13088083e-042\n",
      " 9.72821400e-072 8.54279957e-072 1.31505890e-047 7.96464489e-063\n",
      " 1.39872067e-076 3.27676475e+179 1.39736838e-076 4.27481516e-033\n",
      " 1.38133825e-047 8.23007388e-067 2.31778498e-052 3.22444660e-086\n",
      " 1.20076529e-071 4.85485105e-033 9.16221641e-072 2.90068163e-057\n",
      " 2.15665907e-048 3.21866557e-057 3.76366927e+179 2.52456485e-052\n",
      " 1.65983111e-076 1.68899158e-052 5.05116837e-038 2.17698521e-076\n",
      " 3.82845837e-086 3.11774661e-033 5.20461289e-090 8.15762487e-043\n",
      " 2.90130635e-057 1.88517055e-259 1.57145322e-076 3.69722024e-057\n",
      " 1.30352776e-076 3.31412176e-033 5.88461180e-062 9.16221641e-072\n",
      " 2.90068163e-057 2.72208644e-077 8.23156243e-067 1.13035537e-042\n",
      " 7.25904207e-043 3.12377104e-033 2.71997800e-077 1.57246000e-076\n",
      " 1.48509022e-076 1.39736839e-076 2.09029145e-076 2.25968496e-264\n",
      " 7.86639781e-067 1.03118807e-071 2.90809397e-033 1.42560360e-071\n",
      " 1.26931504e-076 8.60389793e-067 2.10857538e-052 2.83002697e-077\n",
      " 2.63182478e-052 4.47266847e-033 1.39804065e-076 8.60682409e-067\n",
      " 1.56157566e-259 2.42382273e-052 2.31901959e-052 1.39736838e-076\n",
      " 6.81903770e-038 1.56189077e-259 1.65882564e-076 1.45056494e-047\n",
      " 7.85914194e-067 1.65881646e-076 3.22522900e-086 4.01272471e-057\n",
      " 1.04085015e-042 6.00600476e-067 1.72586180e-047 1.39736839e-076\n",
      " 4.46740222e-033 3.00497380e-264 4.47042320e-033 2.10938304e-052\n",
      " 3.93324235e-062 7.26259327e-043 2.72038984e-077 2.10938146e-052\n",
      " 2.00359377e-076 7.12020981e-067 3.53852853e-057 1.30352776e-076\n",
      " 5.64121159e-038 1.20010287e-071 1.48440892e-076 1.65781098e-076\n",
      " 1.39736838e-076 6.81671933e-038 1.72353426e-259 2.00662064e-052\n",
      " 1.04102394e-042 1.39736838e-076 1.42737834e-071 1.64223677e-259\n",
      " 9.05453100e-043 1.69217462e-052 1.39736838e-076 8.24314809e-067\n",
      " 1.72416694e-259 1.08499378e-042 6.01180051e-067 1.39736838e-076\n",
      " 4.27481516e-033 1.38133825e-047 1.39804460e-076 1.83154244e-076\n",
      " 1.39736838e-076 4.27481516e-033 1.38133825e-047 2.11019710e-052\n",
      " 6.75653387e-067 1.30352776e-076 3.05874624e-057 1.24449584e-047\n",
      " 3.37983674e-057 1.04137701e-042 1.30352776e-076 4.85561802e-033\n",
      " 1.83154902e-076 1.44975788e-047 3.37548068e-057 3.22444662e-086\n",
      " 6.81902860e-038 5.15345312e-062 7.12020981e-067 8.23301709e-067\n",
      " 1.30352777e-076 2.89944183e-057 3.69627789e-033 2.31819118e-052\n",
      " 3.93802039e-062 8.54943206e-072 1.08801059e-071 1.26931437e-076\n",
      " 4.90748812e-062 4.42323277e-062 3.22446201e-086 4.91131607e-062\n",
      " 1.91722550e-076 9.16219920e-072 3.37674233e-057 2.71955959e-077\n",
      " 1.08645867e-071 5.63771216e-062 1.39736839e-076 7.86208524e-067\n",
      " 2.88044097e-264 3.37921687e-057 5.64150677e-062 1.39736839e-076\n",
      " 3.50368784e-033 6.82131103e-038 1.25580961e-071 1.65949115e-076\n",
      " 3.22522897e-086 5.88743457e-062 1.69013224e-052 1.10764400e-047\n",
      " 7.71804259e-043 3.22524744e-086 1.25581655e-071 1.72452020e-047\n",
      " 4.46814862e-033 4.08071454e-033 1.30352842e-076 3.88962321e-033\n",
      " 5.34849928e-038 4.01396199e-057 6.82244550e-038 1.30352841e-076\n",
      " 1.38294713e-047 7.71622963e-043 1.17633406e-047 6.52857656e-038\n",
      " 1.30352841e-076 1.83019434e-076 2.21297068e-052 1.17633406e-047\n",
      " 6.52857656e-038 1.30352841e-076 4.75732173e-038 5.64245261e-062\n",
      " 6.74784033e-067 1.17660760e-047 1.39736839e-076 2.21419893e-052\n",
      " 2.87995449e-264 7.48823831e-067 9.74374159e-072 1.39736839e-076\n",
      " 5.64530873e-062 6.12768349e-062 2.31696769e-052 9.50643629e-043\n",
      " 1.30352841e-076 1.69176197e-052 4.18203419e-062 6.37547076e-067\n",
      " 1.51818485e-047 3.22524433e-086 6.12769467e-062 1.79658425e-052\n",
      " 1.65712833e-076 9.96534087e-043 3.82847377e-086 3.11774661e-033\n",
      " 5.20461289e-090 1.39804065e-076 5.88838408e-062 1.72353923e-259\n",
      " 6.37986290e-067 3.31486240e-033 1.30352776e-076 9.35297829e-067\n",
      " 3.50895702e-033 7.70747122e-043 8.24025595e-067 3.22444967e-086\n",
      " 4.91131607e-062 1.91722550e-076 7.25904207e-043 3.50367902e-033\n",
      " 2.72165983e-077 6.01760739e-067 3.85341799e-057 1.39804065e-076\n",
      " 3.37610541e-057 2.50537181e-264 1.08498899e-042 1.45136781e-047\n",
      " 1.39736839e-076 1.42648967e-071 3.00157413e-264 3.69783282e-057\n",
      " 6.12957152e-062 1.39736839e-076 2.00494050e-076 2.75493333e-264\n",
      " 5.93623745e-038 7.48968733e-067 1.39736839e-076 3.70080655e-033\n",
      " 2.32023834e-052 2.90191403e-057 6.23126357e-038 3.22524737e-086\n",
      " 6.01327201e-067 7.71452582e-043 1.10764400e-047 5.63772332e-062\n",
      " 2.82749853e-077 1.91655342e-076 1.31396041e-071 3.93513459e-062\n",
      " 1.14416124e-071 2.82791858e-077 1.79822037e-052 2.74073553e-057\n",
      " 3.50292668e-033 1.03979646e-042 3.22523824e-086 1.13018634e-042\n",
      " 3.31034848e-033 3.93514198e-062 5.94313869e-038 2.83045039e-077\n",
      " 2.63345770e-052 3.89488946e-033 1.39736838e-076 1.72345528e-047\n",
      " 1.88708567e-259 4.18107722e-062 3.50970348e-033 1.30352775e-076\n",
      " 9.35297829e-067 3.50895702e-033 1.02941763e-071 1.31396300e-071\n",
      " 1.30352777e-076 9.05627594e-043 8.60957100e-043 3.93514198e-062\n",
      " 3.50671181e-033 2.72039806e-077 8.23156243e-067 1.13035537e-042\n",
      " 1.39804065e-076 5.15059323e-062 2.87800673e-264 1.17741155e-047\n",
      " 4.67199809e-062 1.30352842e-076 6.52631640e-038 2.42463521e-052\n",
      " 1.20098722e-071 5.87983801e-062 3.82847064e-086 3.11774661e-033\n",
      " 5.20461289e-090 8.15762487e-043 2.90130635e-057 1.88517055e-259\n",
      " 3.31485348e-033 1.39906589e-076 5.04889871e-038 9.73490693e-072\n",
      " 1.68899158e-052 6.75943180e-067 9.50813983e-043 4.43092073e-086\n",
      " 5.93739888e-038 5.20749306e-090 1.39804065e-076 3.69596597e-057\n",
      " 2.13611181e-264 1.38374895e-047 1.31398878e-047 2.17462751e-076\n",
      " 1.48744112e-076 7.25416364e-043 2.90191403e-057 6.23126357e-038\n",
      " 3.22524737e-086 1.51844797e-047 3.31485645e-033 2.42178835e-052\n",
      " 1.48610098e-076 9.15779348e-072 1.08734475e-071 3.22446501e-086\n",
      " 1.38160867e-047 8.16465861e-043 1.91790279e-076 5.88080242e-062\n",
      " 6.00165151e-067 3.69553439e-033 2.82792685e-077 3.12377105e-033\n",
      " 1.83154114e-076 1.79535115e-052 1.31417631e-071 1.30352842e-076\n",
      " 7.86205113e-067 2.87800104e-264 1.31351911e-071 7.11732356e-067\n",
      " 3.11773483e-033 2.63344500e-052 2.72207821e-077 5.64582572e-038\n",
      " 4.18300973e-062 4.27405097e-033 1.17847541e-047 1.30352775e-076\n",
      " 9.72819646e-072 1.72258774e-259 4.01336158e-057 2.58455002e-057\n",
      " 7.11730071e-067 8.60103418e-067 1.30352841e-076 7.72327704e-043\n",
      " 2.88043908e-264 1.72559558e-047 4.27631390e-033 1.39804065e-076\n",
      " 3.06246543e-057 2.87945669e-264 5.64810358e-038 7.11735166e-067\n",
      " 1.39736839e-076 1.40007665e-076 2.25823499e-264 2.00325637e-076\n",
      " 8.23301718e-067 9.15779348e-072 2.00661589e-052 2.71997965e-077\n",
      " 2.62978246e-052 7.12315874e-067 1.31344688e-047 8.60680161e-067\n",
      " 1.30352777e-076 9.50291233e-043 1.88740202e-259 9.74595245e-072\n",
      " 9.16892709e-072 4.90748073e-062 4.18583245e-062 1.30352841e-076\n",
      " 1.13088291e-042 3.12757208e-264 7.86641490e-067 4.42512451e-062\n",
      " 6.37545948e-067 1.74551679e-076 3.22445888e-086 2.00335482e-052\n",
      " 1.19965984e-071 6.81440990e-038 6.01039685e-067 3.22444663e-086\n",
      " 1.10818380e-047 7.26082128e-043 2.00334205e-052 3.38109837e-057\n",
      " 3.22444964e-086 2.74137239e-057 1.31351909e-071 5.94313408e-038\n",
      " 1.79617959e-052 3.37547576e-057 5.15155393e-062 3.22524441e-086\n",
      " 3.53542192e-057 5.05462571e-038 5.64246364e-062 2.00460712e-076\n",
      " 6.00165154e-067 2.09130222e-076 3.22444668e-086 1.20098551e-071\n",
      " 1.14327690e-071 3.85714201e-057 4.18582509e-062 1.30352776e-076\n",
      " 3.12002419e-033 8.23879572e-067 2.11019710e-052 6.75653387e-067\n",
      " 1.30352776e-076 7.49839781e-067 2.82750346e-077 2.63140581e-052\n",
      " 2.89756043e-057 6.00453887e-067 7.27308933e-043 2.82750513e-077\n",
      " 1.17741155e-047 3.21866312e-057 9.72817914e-072 2.90066951e-057\n",
      " 3.22524434e-086 1.31452333e-047 1.89894201e-052 5.63674775e-062\n",
      " 8.16639657e-043 3.82848294e-086 3.11774661e-033 5.20461289e-090\n",
      " 1.39804065e-076 1.83052645e-076 2.04936434e-259 1.57145322e-076\n",
      " 3.69722024e-057 8.54943213e-072 1.08801059e-071 1.26931437e-076\n",
      " 9.34288662e-067 2.10775176e-052 2.72207487e-077 9.50815348e-043\n",
      " 7.41135408e-038 2.17462751e-076 1.48744112e-076 7.25416364e-043\n",
      " 2.52536960e-052 1.90099390e-052 3.82847376e-086 3.11774661e-033\n",
      " 5.20461289e-090 8.15762487e-043 2.90130635e-057 1.88517055e-259\n",
      " 1.57145322e-076 3.69722024e-057 8.54943213e-072 1.65609952e-047\n",
      " 1.26931702e-076 9.34288662e-067 2.10775176e-052 2.15664990e-048\n",
      " 3.21866557e-057 3.76366927e+179 2.17462751e-076 1.48744112e-076\n",
      " 7.25416364e-043 2.90191403e-057 6.23126357e-038 3.82847678e-086\n",
      " 3.11774661e-033 5.20461289e-090 8.15762487e-043 2.90130635e-057\n",
      " 1.88517055e-259 1.57145322e-076 3.69722024e-057 8.54943213e-072\n",
      " 1.65609952e-047 1.26931702e-076 9.34288662e-067 2.10775176e-052\n",
      " 2.15664990e-048 3.21866557e-057 3.76366927e+179 2.17462751e-076\n",
      " 1.48744112e-076 7.25416364e-043 2.90191403e-057 6.23126357e-038\n",
      " 3.82847678e-086 3.11774661e-033 5.20461289e-090 8.15762487e-043\n",
      " 2.90130635e-057 1.88517055e-259 1.57145322e-076 3.69722024e-057\n",
      " 8.54943213e-072 1.65609952e-047 1.26931702e-076 9.34288662e-067\n",
      " 2.10775176e-052 2.15664990e-048 3.21866557e-057 3.76366927e+179\n",
      " 2.17462751e-076 1.48744112e-076 7.25416364e-043 2.90191403e-057\n",
      " 6.23126357e-038 3.82847678e-086 3.11774661e-033 5.20461289e-090\n",
      " 8.15762487e-043 2.90130635e-057 1.88517055e-259 1.57145322e-076\n",
      " 3.69722024e-057 8.54943213e-072 1.65609952e-047 1.26931702e-076\n",
      " 9.34288662e-067 2.10775176e-052 2.15664990e-048 3.21866557e-057\n",
      " 3.76366927e+179 2.17462751e-076 1.48744112e-076 7.25416364e-043\n",
      " 2.90191403e-057 6.23126357e-038 3.82847678e-086 3.11774661e-033\n",
      " 5.20461289e-090 8.15762487e-043 2.90130635e-057 1.88517055e-259\n",
      " 1.57145322e-076 3.69722024e-057 8.54943213e-072 1.65609952e-047\n",
      " 1.26931702e-076 9.34288662e-067 2.10775176e-052 2.15664990e-048\n",
      " 3.21866557e-057 3.76366927e+179 2.17462751e-076 1.48744112e-076\n",
      " 7.25416364e-043 2.52536960e-052 1.90099390e-052 3.82847376e-086\n",
      " 3.11774661e-033 5.20461289e-090 8.15762487e-043 2.90130635e-057\n",
      " 1.88517055e-259 1.57145322e-076 3.69722024e-057 8.54943213e-072\n",
      " 1.65609952e-047 1.26931702e-076 9.34288662e-067 2.10775176e-052\n",
      " 2.15664990e-048 3.21866557e-057 3.76366927e+179 2.17462751e-076\n",
      " 1.48744112e-076 7.25416364e-043 2.90191403e-057 6.23126357e-038\n",
      " 3.82847678e-086 3.11774661e-033 5.20461289e-090 8.15762487e-043\n",
      " 2.90130635e-057 1.88517055e-259 1.57145322e-076 3.69722024e-057\n",
      " 8.54943213e-072 1.65609952e-047 1.26931702e-076 9.34288662e-067\n",
      " 2.10775176e-052 2.15664990e-048 3.21866557e-057 3.76366927e+179\n",
      " 2.17462751e-076 1.48744112e-076 7.25416364e-043 2.90191403e-057\n",
      " 6.23126357e-038 3.82847678e-086 3.11774661e-033 5.20461289e-090\n",
      " 8.15762487e-043 2.90130635e-057 1.88517055e-259 1.57145322e-076\n",
      " 3.69722024e-057 8.54943213e-072 1.65609952e-047 1.26931702e-076\n",
      " 9.34288662e-067 2.10775176e-052 2.15664990e-048 3.21866557e-057\n",
      " 3.76366927e+179 2.17462751e-076 1.48744112e-076 7.25416364e-043\n",
      " 2.90191403e-057 6.23126357e-038 3.82847678e-086 3.11774661e-033\n",
      " 5.20461289e-090 8.15762487e-043 2.90130635e-057 1.88517055e-259\n",
      " 1.57145322e-076 3.69722024e-057 8.54943213e-072 1.65609952e-047\n",
      " 1.26931702e-076 9.34288662e-067 2.10775176e-052 2.15664990e-048\n",
      " 3.21866557e-057 3.76366927e+179 2.17462751e-076 1.48744112e-076\n",
      " 7.25416364e-043 2.90191403e-057 6.23126357e-038 3.82847678e-086\n",
      " 3.11774661e-033 5.20461289e-090 1.39804065e-076 1.69013543e-052\n",
      " 1.39738168e-259 1.57145322e-076 3.69722024e-057 8.54943213e-072\n",
      " 1.65609952e-047 1.26931702e-076 9.34288662e-067 2.10775176e-052\n",
      " 2.15664990e-048 3.21866557e-057 3.76366927e+179 2.17462751e-076\n",
      " 1.48744112e-076 7.25416364e-043 2.90191403e-057 6.23126357e-038\n",
      " 3.82847678e-086 3.11774661e-033 5.20461289e-090 8.15762487e-043\n",
      " 2.90130635e-057 1.88517055e-259 1.57145322e-076 3.69722024e-057\n",
      " 8.54943213e-072 1.65609952e-047 1.26931702e-076 9.34288662e-067\n",
      " 2.10775176e-052 2.15664990e-048 3.21866557e-057 3.76366927e+179\n",
      " 2.17462751e-076 1.48744112e-076 7.25416364e-043 2.90191403e-057\n",
      " 6.23126357e-038 3.82847678e-086 3.11774661e-033 5.20461289e-090\n",
      " 8.15762487e-043 2.90130635e-057 1.88517055e-259 1.57145322e-076\n",
      " 3.69722024e-057 8.54943213e-072 1.65609952e-047 1.26931702e-076\n",
      " 9.34288662e-067 2.10775176e-052 2.15664990e-048 3.21866557e-057\n",
      " 3.76366927e+179 2.17462751e-076 1.48744112e-076 7.25416364e-043\n",
      " 2.90191403e-057 6.23126357e-038 3.82847678e-086 3.11774661e-033\n",
      " 5.20461289e-090 8.15762487e-043 2.90130635e-057 1.88517055e-259\n",
      " 1.57145322e-076 3.69722024e-057 8.54943213e-072 1.65609952e-047\n",
      " 1.26931702e-076 9.34288662e-067 2.10775176e-052 2.15664990e-048\n",
      " 3.21866557e-057 3.76366927e+179 2.17462751e-076 1.48744112e-076\n",
      " 7.25416364e-043 2.90191403e-057 6.23126357e-038 3.82847678e-086\n",
      " 3.11774661e-033 5.20461289e-090 8.15762487e-043 2.90130635e-057\n",
      " 1.88517055e-259 1.57145322e-076 3.69722024e-057 8.54943213e-072\n",
      " 1.65609952e-047 1.26931702e-076 9.34288662e-067 2.10775176e-052\n",
      " 2.15664990e-048 3.21866557e-057 3.76366927e+179 2.17462751e-076\n",
      " 1.48744112e-076 7.25416364e-043 2.90191403e-057 6.23126357e-038\n",
      " 3.82847678e-086 3.11774661e-033 5.20461289e-090 8.15762487e-043\n",
      " 2.90130635e-057 1.88517055e-259 1.57145322e-076 3.69722024e-057\n",
      " 8.54943213e-072 1.65609952e-047 1.26931702e-076 9.34288662e-067\n",
      " 2.10775176e-052 2.15664990e-048 3.21866557e-057 3.76366927e+179\n",
      " 2.17462751e-076 1.48744112e-076 7.25416364e-043 2.90191403e-057\n",
      " 6.23126357e-038 3.82847678e-086 3.11774661e-033 5.20461289e-090\n",
      " 8.15762487e-043 2.90130635e-057 1.88517055e-259 1.57145322e-076\n",
      " 3.69722024e-057 8.54943213e-072 1.65609952e-047 1.26931702e-076\n",
      " 9.34288662e-067 2.10775176e-052 2.15664990e-048 3.21866557e-057\n",
      " 3.76366927e+179 2.17462751e-076 1.48744112e-076 7.25416364e-043\n",
      " 2.90191403e-057 6.23126357e-038 3.82847678e-086 3.11774661e-033\n",
      " 5.20461289e-090 8.15762487e-043 2.90130635e-057 1.88517055e-259\n",
      " 1.57145322e-076 3.69722024e-057 8.54943213e-072 1.65609952e-047\n",
      " 1.26931702e-076 9.34288662e-067 2.10775176e-052 2.15664990e-048\n",
      " 3.21866557e-057 3.76366927e+179 2.17462751e-076 1.48744112e-076\n",
      " 7.25416364e-043 2.90191403e-057 6.23126357e-038 3.82847678e-086\n",
      " 3.11774661e-033 5.20461289e-090 8.15762487e-043 2.90130635e-057\n",
      " 1.88517055e-259 1.57145322e-076 3.69722024e-057 8.54943213e-072\n",
      " 1.65609952e-047 1.26931702e-076 9.34288662e-067 2.10775176e-052\n",
      " 2.15664990e-048 3.21866557e-057 3.76366927e+179 2.17462751e-076\n",
      " 1.48744112e-076 7.25416364e-043 2.90191403e-057 6.23126357e-038\n",
      " 3.82847678e-086 3.11774661e-033 5.20461289e-090 8.15762487e-043\n",
      " 2.90130635e-057 1.88517055e-259 1.57145322e-076 3.69722024e-057\n",
      " 8.54943213e-072 1.65609952e-047 1.26931702e-076 9.34288662e-067\n",
      " 2.10775176e-052 2.15664990e-048 3.21866557e-057 3.76366927e+179\n",
      " 2.17462751e-076 1.48744112e-076 7.25416364e-043 2.90191403e-057\n",
      " 6.23126357e-038 3.82847678e-086 3.11774661e-033 5.20461289e-090\n",
      " 8.15762487e-043 2.90130635e-057 1.88517055e-259 1.57145322e-076\n",
      " 3.69722024e-057 8.54943213e-072 1.65609952e-047 1.26931702e-076\n",
      " 9.34288662e-067 2.10775176e-052 2.15664990e-048 3.21866557e-057\n",
      " 3.76366927e+179 2.17462751e-076 1.48744112e-076 7.25416364e-043\n",
      " 2.90191403e-057 6.23126357e-038 3.82847678e-086 3.11774661e-033\n",
      " 5.20461289e-090 8.15762487e-043 2.90130635e-057 1.88517055e-259\n",
      " 1.57145322e-076 3.69722024e-057 8.54943213e-072 1.65609952e-047\n",
      " 1.26931702e-076 9.34288662e-067 2.10775176e-052 2.15664990e-048\n",
      " 3.21866557e-057 3.76366927e+179 2.17462751e-076 1.48744112e-076\n",
      " 7.25416364e-043 2.90191403e-057 6.23126357e-038 3.82847678e-086\n",
      " 3.11774661e-033 5.20461289e-090 8.15762487e-043 2.90130635e-057\n",
      " 1.88517055e-259 1.57145322e-076 3.69722024e-057 8.54943213e-072\n",
      " 1.65609952e-047 1.26931702e-076 9.34288662e-067 2.10775176e-052\n",
      " 2.15664990e-048 3.21866557e-057 3.76366927e+179 2.17462751e-076\n",
      " 1.48744112e-076 7.25416364e-043 2.90191403e-057 6.23126357e-038\n",
      " 3.82847678e-086 3.11774661e-033 5.20461289e-090 8.15762487e-043\n",
      " 2.90130635e-057 1.88517055e-259 1.57145322e-076 3.69722024e-057\n",
      " 8.54943213e-072 1.65609952e-047 1.26931702e-076 9.34288662e-067\n",
      " 2.10775176e-052 2.15664990e-048 3.21866557e-057 3.76366927e+179\n",
      " 2.17462751e-076 1.48744112e-076 7.25416364e-043 2.90191403e-057\n",
      " 6.23126357e-038 3.82847678e-086 3.11774661e-033 5.20461289e-090\n",
      " 8.15762487e-043 2.90130635e-057 1.88517055e-259 1.57145322e-076\n",
      " 3.69722024e-057 8.54943213e-072 1.65609952e-047 1.26931702e-076\n",
      " 9.34288662e-067 2.10775176e-052 2.15664990e-048 3.21866557e-057\n",
      " 3.76366927e+179 2.17462751e-076 1.48744112e-076 7.25416364e-043\n",
      " 2.90191403e-057 6.23126357e-038 3.82847678e-086 3.11774661e-033\n",
      " 5.20461289e-090 8.15762487e-043 2.90130635e-057 1.88517055e-259\n",
      " 1.57145322e-076 3.69722024e-057 8.54943213e-072 1.65609952e-047\n",
      " 1.26931702e-076 9.34288662e-067 2.10775176e-052 2.15664990e-048\n",
      " 3.21866557e-057 3.76366927e+179 2.17462751e-076 1.48744112e-076\n",
      " 7.25416364e-043 2.90191403e-057 6.23126357e-038 3.82847678e-086\n",
      " 3.11774661e-033 5.20461289e-090 8.15762487e-043 2.90130635e-057\n",
      " 1.88517055e-259 1.57145322e-076 3.69722024e-057 8.54943213e-072\n",
      " 1.65609952e-047 1.26931702e-076 9.34288662e-067 2.10775176e-052\n",
      " 2.15664990e-048 3.21866557e-057 3.76366927e+179 2.17462751e-076\n",
      " 1.48744112e-076 7.25416364e-043 2.90191403e-057 6.23126357e-038\n",
      " 3.82847678e-086 3.11774661e-033 5.20461289e-090 8.15762487e-043\n",
      " 2.90130635e-057 1.88517055e-259 1.57145322e-076 3.69722024e-057\n",
      " 8.54943213e-072 1.65609952e-047 1.26931702e-076 9.34288662e-067\n",
      " 2.10775176e-052 2.15664990e-048 3.21866557e-057 3.76366927e+179\n",
      " 2.17462751e-076 1.48744112e-076 7.25416364e-043 2.90191403e-057\n",
      " 6.23126357e-038 3.82847678e-086 3.11774661e-033 5.20461289e-090\n",
      " 8.15762487e-043 2.90130635e-057 1.88517055e-259 1.57145322e-076\n",
      " 3.69722024e-057 8.54943213e-072 1.65609952e-047 1.26931702e-076\n",
      " 9.34288662e-067 2.10775176e-052 2.15664990e-048 3.21866557e-057\n",
      " 3.76366927e+179 2.17462751e-076 1.48744112e-076 7.25416364e-043\n",
      " 2.90191403e-057 6.23126357e-038 3.82847678e-086 3.11774661e-033\n",
      " 5.20461289e-090 8.15762487e-043 2.90130635e-057 1.88517055e-259\n",
      " 1.57145322e-076 3.69722024e-057 8.54943213e-072 1.65609952e-047\n",
      " 1.26931702e-076 9.34288662e-067 2.10775176e-052 2.15664990e-048\n",
      " 3.21866557e-057 3.76366927e+179 2.17462751e-076 1.48744112e-076\n",
      " 7.25416364e-043 2.90191403e-057 6.23126357e-038 3.82847678e-086\n",
      " 3.11774661e-033 5.20461289e-090 8.15762487e-043 2.90130635e-057\n",
      " 1.88517055e-259 1.57145322e-076 3.69722024e-057 8.54943213e-072\n",
      " 1.65609952e-047 1.26931702e-076 9.34288662e-067 2.10775176e-052\n",
      " 2.15664990e-048 3.21866557e-057 3.76366927e+179 2.17462751e-076\n",
      " 1.48744112e-076 7.25416364e-043 2.90191403e-057 6.23126357e-038\n",
      " 3.82847678e-086 3.11774661e-033 5.20461289e-090 8.15762487e-043\n",
      " 2.90130635e-057 1.88517055e-259 1.57145322e-076 3.69722024e-057\n",
      " 8.54943213e-072 1.65609952e-047 1.26931702e-076 9.34288662e-067\n",
      " 2.10775176e-052 2.15664990e-048 3.21866557e-057 3.76366927e+179\n",
      " 2.17462751e-076 1.48744112e-076 7.25416364e-043 2.90191403e-057\n",
      " 6.23126357e-038 3.82847678e-086 3.11774661e-033 5.20461289e-090\n",
      " 8.15762487e-043 2.90130635e-057 1.88517055e-259 1.57145322e-076\n",
      " 3.69722024e-057 8.54943213e-072 1.65609952e-047 1.26931702e-076\n",
      " 9.34288662e-067 2.10775176e-052 2.15664990e-048 3.21866557e-057\n",
      " 3.76366927e+179 2.17462751e-076 1.48744112e-076 7.25416364e-043\n",
      " 2.90191403e-057 6.23126357e-038 3.82847678e-086 3.11774661e-033\n",
      " 5.20461289e-090 8.15762487e-043 2.90130635e-057 1.88517055e-259]\n"
     ]
    }
   ],
   "source": [
    "B = np.fromfile('PerlScriptAndModel/P.text',dtype=float)\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1320,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 2)\n"
     ]
    }
   ],
   "source": [
    "## Extracting the probabilities of occurrence of each character (ASCII codes) in the header and in the body.\n",
    "##and building the matrix B\n",
    "B = np.empty((0,2))\n",
    "\n",
    "with open('PerlScriptAndModel/P.text') as f:\n",
    "    for line in f:\n",
    "        row = line.split('\\t')\n",
    "        row = [float(i) for i in row]\n",
    "        B = np.vstack([B, row])\n",
    "        \n",
    "print(B.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of Viterbi algorithm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Viterbi path is the most likely sequence of hidden states that produce a sequence of observed events. We can calculate Viterbi path using Viterbi algorithm.\n",
    "Finding Viterbi path provides the answer to decoding question in HMM â€“ given observations and states transitions find most likely states.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will applicate the log function to transition matrix and the vector of Initial state probabilities because we will have too small probabilities that can be considerated as null values by the executor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(V, A, B, Pi):\n",
    "    \"\"\"\n",
    "    Return the MAP estimate of state trajectory of Hidden Markov Model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    V : array (T,)\n",
    "        Observation state sequence. int dtype.\n",
    "    A : array (M, M)\n",
    "        State transition matrix. See HiddenMarkovModel.state_transition  for\n",
    "        details.\n",
    "    B : array (M, 256)\n",
    "        Emission matrix. See HiddenMarkovModel.emission for details.\n",
    "    Pi: optional, (M,)\n",
    "        Initial state probabilities: Pi[i] is the probability x[0] == i. If\n",
    "        None, uniform initial distribution is assumed (Pi[:] == 1/K).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    path : array (T,)\n",
    "        Maximum a posteriori probability estimate of hidden state trajectory,\n",
    "        conditioned on observation sequence y under the model parameters A, B,\n",
    "        Pi.\n",
    "    \n",
    "    \"\"\"\n",
    "    T = V.shape[0]      #Number of observations\n",
    "    M = A.shape[0]      #Number of states\n",
    " \n",
    "    T1 = np.zeros((T, M))\n",
    "     # Initilaize the tracking tables from first observation\n",
    "    T1[0, :] = np.log(Pi * B[:, V[0]])\n",
    "    backTrack = np.zeros((T - 1, M))\n",
    " \n",
    "    # Iterate throught the observations updating the tracking tables\n",
    "    for t in range(1, T):\n",
    "        for j in range(M):\n",
    "            \n",
    "            probability = T1[t - 1] + np.log(A[:, j]) + np.log(B[j, V[t]])\n",
    "            backTrack[t - 1, j] = np.argmax(probability)  # This is our most probable state given \n",
    "                                                          #previous state at time t (1)\n",
    "            T1[t, j] = np.max(probability)                # This is the probability of the most probable state (2)\n",
    " \n",
    "    # Path Array\n",
    "    path = np.zeros(T)\n",
    " \n",
    "    # Find the most probable last hidden state\n",
    "    last_state = np.argmax(T1[T - 1, :])\n",
    " \n",
    "    path[0] = last_state\n",
    " \n",
    "    backtrack_index = 1\n",
    "    for i in range(T - 2, -1, -1):\n",
    "        path[backtrack_index] = backTrack[i, int(last_state)]\n",
    "        last_state = backTrack[i, int(last_state)]\n",
    "        backtrack_index += 1\n",
    " \n",
    "    \n",
    "    path = np.flip(path, axis=0)\n",
    " \n",
    "    return path "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test of Viterbi algorithm using mail samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahlem\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:31: RuntimeWarning: divide by zero encountered in log\n",
      "C:\\Users\\ahlem\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:38: RuntimeWarning: divide by zero encountered in log\n"
     ]
    }
   ],
   "source": [
    "## Using mail11.dat\n",
    "sample=[]\n",
    "with open('dat/mail11.dat') as f : #open the file that contains the email\n",
    "    for line in f:                 #pass throw every line\n",
    "        sample.append(int(line))   #convert line into int and save the value\n",
    "sample=np.array(sample)\n",
    "\n",
    "path=viterbi(sample, A, B.transpose(), Pi)\n",
    "\n",
    "# Convert numeric values to actual hidden states\n",
    "result = []\n",
    "for s in path:\n",
    "    if s == 0:\n",
    "        result.append(1)\n",
    "    else:\n",
    "        result.append(2)\n",
    "print(result)\n",
    "##Printing the sequence on file path_11.txt \n",
    "with open('path_11.txt', 'w') as f:\n",
    "    for item in result:\n",
    "        f.write(\"%s\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahlem\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:31: RuntimeWarning: divide by zero encountered in log\n",
      "C:\\Users\\ahlem\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:38: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "## Using mail11.dat\n",
    "sample=[]\n",
    "with open('dat/mail30.dat') as f : #open the file that contains the email\n",
    "    for line in f:                 #pass throw every line\n",
    "        sample.append(int(line))   #convert line into int and save the value\n",
    "sample=np.array(sample)\n",
    "\n",
    "path=viterbi(sample, A, B.transpose(), Pi)\n",
    "\n",
    "# Convert numeric values to actual hidden states\n",
    "result = []\n",
    "for s in path:\n",
    "    if s == 0:\n",
    "        result.append(1)\n",
    "    else:\n",
    "        result.append(2)\n",
    "print(result)\n",
    "\n",
    "##Printing the sequence on file path2.txt \n",
    "with open('path2.txt', 'w') as f:\n",
    "    for item in result:\n",
    "        f.write(\"%s\" % item)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will use Perl to visualize the segmentation produced by our segmenter in the form of the best path found by the Viterbi algorithm (in a vector of 1 and 2).\n",
    "\n",
    "1-\tInstalling Perl via https://learn.perl.org/installing/windows.html.\n",
    "\n",
    "2-\tLaunching perl using the command line : perl segment.pl mail.txt path.txt\n",
    "\n",
    "As we see in the images bellow, the two parts (header and body) of each mail are separated by the setence \"======= couper ici =======\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result obtained on mail30.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"cut_mail30.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"cut_mail30_2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result obtained on mail11.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"cut_mail_11.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The body of mail11.txt is smaller then the body of mail30.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q5 : How would you model the problem if you had to segment the mails in more than two parts (for example : header, body, signature) ? Draw a diagram of the corresponding Hidden Markov model and give an example of A matrix that would be suitable in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we will have 3 parts in the mails, It is proposed to perform this task by learning a HMM (A;B; $\\pi$) with three states, one (state 1) for the header, the second (state 2) for the body and the third (state 3) for the signature.\n",
    "\n",
    "Knowing that each mail contains exactly one header, one body and one signature, each mail follows once the transition from 1 to 2 and form 2 to 3.\n",
    "\n",
    "The transition matrice A will be a 3*3 matrix. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$A = \\quad \n",
    "\\begin{pmatrix} \n",
    " 0.99 & 0.008 & 0.002 \\\\\n",
    "0 & 0.99 & 0.001 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{pmatrix}\n",
    "\\quad$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <b> P(1/1)=0 : </b>  If we are the state one some we have a probability to remain in the same state.\n",
    "\n",
    "<b> P(2/1)=0.008 and P(3/1)=0.002 : </b> After an important number of state 1 we will pass to the state 2  or state 3. (most probably state 2)\n",
    "\n",
    "<b>P(1/2)=0 , P(1/3)=0, and P(2/3)=0 : </b> Impossible transitions\n",
    "\n",
    "<b>P(3/3) =1 :</b> Once we encounter the state 3 we canâ€™t change to any other state. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hidden Markov Chain is :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"HMM1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q6 : How would you model the problem of separating the portions of mail included, knowing that they always start with the character \">\". Draw a diagram of the corresponding Hidden Markov model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new hidden states are :\n",
    "-\tState 1 : â€˜>â€™\n",
    "-\tState 2 : header\n",
    "-\tState 3 : Body\n",
    "-\tState 4 : Signature \n",
    "\n",
    "We will add a new state that correspond to the character â€˜>â€™. The other states can move only to it. And this state can move to all other states.\n",
    "\n",
    "We canâ€™t move from the last state to the state 1 because once we are in the last portions of the mail we canâ€™t back to any other portion.\n",
    "\n",
    "\n",
    "\n",
    "The hidden Markov Chain is :\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"HMM.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
